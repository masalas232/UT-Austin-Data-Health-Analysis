{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Quality Analysis\n",
    "\n",
    "The data set is not presented in way that would pass validation standards for USPS or email format, this is simply information with which to work.\n",
    "\n",
    "The analysis presented here has three purposes: \n",
    "\n",
    "- Create metrics that show the health* of addresses, phones, and emails for all living individuals in the data set.\n",
    "- Break that analysis out into segments by entity type to show the health* of each entity category.\n",
    "- Note any potential relationships and any data anomalies.\n",
    "\n",
    "*Health indicates the number of instances where the information is valid.\n",
    "\n",
    "The analysis will be carried out using Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will begin by impoting the necessary `pandas` library to work with data sets and load the data set into our working environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import pandas library\n",
    "\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "# Load data set into working environment\n",
    "\n",
    "file_name = \"DQS_dataset.xlsx\"   # save file name as an object\n",
    "\n",
    "dqs_data = pd.read_excel(file_name)  # read data into working environment "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "\n",
    "Now that we have our data set in our working environment I will begin exploring the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'FIRST_NAME', 'MIDDLE_NAME', 'LAST_NAME', 'AGE', 'DECEASED_FLAG',\n",
       "       'ENTITY_TYPE', 'ADDRESS_LINE_1', 'ADDRESS_LINE_2', 'CITY', 'STATE',\n",
       "       'ZIP', 'BAD_ADDR_FLG', 'EMAIL', 'BAD_EMAIL_FLG', 'PHONE',\n",
       "       'BAD_PHONE_FLG'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Explore Column Names\n",
    "\n",
    "dqs_data.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the rows I will work with to conduct our analysis. \n",
    "\n",
    "I will continue by checking the number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120, 17)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dimensions of the DataFrame\n",
    "\n",
    "dqs_data.shape \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 120 rows and 17 columns in the data set. \n",
    "\n",
    "I will continue to explore the number of entries in each column and the type of data found within each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 120 entries, 0 to 119\n",
      "Data columns (total 17 columns):\n",
      "ID                120 non-null int64\n",
      "FIRST_NAME        120 non-null object\n",
      "MIDDLE_NAME       63 non-null object\n",
      "LAST_NAME         120 non-null object\n",
      "AGE               81 non-null float64\n",
      "DECEASED_FLAG     4 non-null object\n",
      "ENTITY_TYPE       120 non-null object\n",
      "ADDRESS_LINE_1    120 non-null object\n",
      "ADDRESS_LINE_2    27 non-null object\n",
      "CITY              120 non-null object\n",
      "STATE             119 non-null object\n",
      "ZIP               116 non-null float64\n",
      "BAD_ADDR_FLG      6 non-null object\n",
      "EMAIL             118 non-null object\n",
      "BAD_EMAIL_FLG     5 non-null object\n",
      "PHONE             102 non-null float64\n",
      "BAD_PHONE_FLG     3 non-null object\n",
      "dtypes: float64(3), int64(1), object(13)\n",
      "memory usage: 16.0+ KB\n"
     ]
    }
   ],
   "source": [
    "# Information about each variable, details \n",
    "\n",
    "dqs_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that a complete column is made up of 120 entries, some columns with less than 120 entries denote missing information, but others are flags raised when the information contained in that particular row is bad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "As mentioned in the beginning, we are only to analyze the health of addresses, phones, and emails for all living individuals. Let's begin by taking out the individuals who are deceased. I will use the `DECEASED_FLAG` column to perform the operation. By looking at the info from our columns, found above,  we that there are four rows flagged as deceased. \n",
    "\n",
    "To make taking out of rows of people who are deceased easier, I will assign a value of 0 to those who are not deceased and a value of 1 to those who are.  \n",
    "\n",
    "The operation will take out the 4 rows that are flagged as being deceased. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign value of 0 to those not deceased(in our case NaN = not deceased), and value of 1 to those who are\n",
    "\n",
    "dqs_data['DECEASED_FLAG'] = dqs_data['DECEASED_FLAG'].fillna(value = 0)  # replace NaN with 0\n",
    "\n",
    "dqs_data['DECEASED_FLAG'] = dqs_data['DECEASED_FLAG'].replace('X', 1)  # replace X with 1\n",
    "\n",
    "\n",
    "# Now we only keep rows where 'DECEASED_FLAG' = 0, not deceased\n",
    "\n",
    "dqs_data = dqs_data.loc[dqs_data['DECEASED_FLAG'] == 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have taken out the rows flagged as deceased, let's check out our new number of rows and columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116, 17)"
      ]
     },
     "execution_count": 298,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Print new number of rows and columns\n",
    "\n",
    "dqs_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have 116 rows, previously 120. We can now continue with testing the quality of our data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality\n",
    "\n",
    "Data quality refers to the ability of a set of data to serve an intended purpose. Low-quality data cannot be used effectively to provide the insights you are looking for. \n",
    "\n",
    "According to [Syncsort](https://blog.syncsort.com/2018/02/data-quality/how-to-measure-data-quality-7-metrics/), there are seven data quality metrics to measure the quality of your data. There are various articles which can be helpful in measuring data quality and what metrics, but they all revolve around the same basic metrics. \n",
    "\n",
    "Given the data that we have available in our dataset, there are three metrics I will use:\n",
    "\n",
    "- Number of missing values\n",
    "- Incorrect data\n",
    "- Ratio of data to errors\n",
    "\n",
    "Let's begin by assesing the quality of the addresses in our data, continued by email and phone numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality/Validity of Addresses\n",
    "\n",
    "The columns that will help us determine the quality are `ADDRESS_LINE_1`, `ADDRESS_LINE_2`, `CITY`, `STATE`, `ZIP`, and `BAD_ADDR_FLG`. \n",
    "\n",
    "I will create a dataset `dqs_addr` to inspect the quality/validity of the address portion of the data and begin by taking a look at the number of missing values and then proceed to inspect the accuracy of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADDRESS_LINE_1      0\n",
       "ADDRESS_LINE_2     89\n",
       "CITY                0\n",
       "STATE               1\n",
       "ZIP                 4\n",
       "BAD_ADDR_FLG      110\n",
       "dtype: int64"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dqs_addr dataset\n",
    "\n",
    "dqs_addr = dqs_data[['ADDRESS_LINE_1', 'ADDRESS_LINE_2', 'CITY', 'STATE', 'ZIP', 'BAD_ADDR_FLG']]\n",
    "\n",
    "\n",
    "# Find the total number of missing values by column\n",
    "\n",
    "dqs_addr.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the columns `ADDRESS_LINE_2` and `BAD_ADDR_FLG` have many missing values, but we shouldn't consider this as true missing data. Not everyone has a second address line (such as apartment or suite number), and missing values in the column flagging a bad address simply means that it is not a bad address. \n",
    "\n",
    "The columns we should consider as having missing values are `STATE` with 1 and `ZIP` with 4 missing values. We can explore these rows with missing values further to understand why they might be missing. \n",
    "\n",
    "For this I will create a dataset `null_s_z` to include only rows with missing values in `STATE` and `ZIP` for further inspection. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            ADDRESS_LINE_1 ADDRESS_LINE_2     CITY      STATE  \\\n",
      "16  No.609 Youhao Street, Jinzhou District            NaN   DALIAN        NaN   \n",
      "45            D65, Sispal Vihar Sohna Road      Sector 49  GURGAON    HARYANA   \n",
      "46                      D22 jinxiu Huayuan            NaN  CHIFENG  NEIMENGGU   \n",
      "82                      D22 jinxiu Huayuan            NaN  CHIFENG  NEIMENGGU   \n",
      "\n",
      "    ZIP BAD_ADDR_FLG  \n",
      "16  NaN          NaN  \n",
      "45  NaN          NaN  \n",
      "46  NaN          NaN  \n",
      "82  NaN          NaN  \n"
     ]
    }
   ],
   "source": [
    "# Dataset for rows with missing values in address section\n",
    "\n",
    "null_s_z = dqs_addr[dqs_addr[['STATE', 'ZIP']].isnull().any(axis=1)]\n",
    "\n",
    "print(null_s_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that all the rows containing missing values for `STATE` and `ZIP` are for people who live outside the US, it is undestandable that addresses for different countries are formatted differently and are not necessarily missing data. Addresses outside of the US could be flagged or put in another dataset altogether to be analyzed on there own. \n",
    "\n",
    "We will continue to asses the validity of our data by analyzing the accuracy of the data found in the addresses. \n",
    "\n",
    "Our dataset contains a column `BAD_ADDR_FLG` which denotes an address that is wrong for that particular person. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Count number of columns with a bad address\n",
    "\n",
    "dqs_addr['BAD_ADDR_FLG'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from counting the number of bad address flag, we already know that we have six addresses that are bad. \n",
    "\n",
    "Let's continue by analyzing the data containted in each row of the `ADDRESS_LINE_1` column to make sure we actually have an address in the row. To do this I will run the through the dataset checking if each value in the `ADDRESS_LINE_1` column starts with a number, as do all addresses in the US. I create a new column `good_addr` which will be True if it does start with a number and False if not.\n",
    "\n",
    "I will then isolate those that do not start with a number to analyze further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new column good_addr to denote addresses starting with a number\n",
    "\n",
    "dqs_addr['good_addr'] = dqs_addr['ADDRESS_LINE_1'].str[0].str.isdigit()\n",
    "\n",
    "\n",
    "# Isolate rows which address doesn't start with a number\n",
    "\n",
    "bad_addr = dqs_addr[dqs_addr['good_addr'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have isolated the rows with addresses not starting with numbers; let's see how many we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 7)\n"
     ]
    }
   ],
   "source": [
    "# Number of rows in bad_addr data set\n",
    "\n",
    "print(bad_addr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 10 rows with bad addresses, let's explore these further. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                             ADDRESS_LINE_1  \\\n",
      "11        The University of Texas at Austin   \n",
      "16   No.609 Youhao Street, Jinzhou District   \n",
      "32        The University of Texas at Austin   \n",
      "45             D65, Sispal Vihar Sohna Road   \n",
      "46                       D22 jinxiu Huayuan   \n",
      "48        The University of Texas at Austin   \n",
      "82                       D22 jinxiu Huayuan   \n",
      "88        The University of Texas at Austin   \n",
      "103                        P. O. Box 276177   \n",
      "111                     .527 Stowers Avenue   \n",
      "\n",
      "                           ADDRESS_LINE_2      CITY      STATE      ZIP  \\\n",
      "11       204 E Dean Keeton St, Stop C2200    Austin         TX  78712.0   \n",
      "16                                    NaN    DALIAN        NaN      NaN   \n",
      "32   2406 Robert Dedman Drive, Stop E3100    Austin         TX  78712.0   \n",
      "45                              Sector 49   GURGAON    HARYANA      NaN   \n",
      "46                                    NaN   CHIFENG  NEIMENGGU      NaN   \n",
      "48              2101 Speedway, Stop D7500    Austin         TX  78712.0   \n",
      "82                                    NaN   CHIFENG  NEIMENGGU      NaN   \n",
      "88              2101 Speedway, Stop D7500    Austin         TX  78712.0   \n",
      "103                                   NaN    Dallas         TX  75381.0   \n",
      "111                                   NaN  Cerritos         CA  90703.0   \n",
      "\n",
      "    BAD_ADDR_FLG  good_addr  \n",
      "11           NaN      False  \n",
      "16           NaN      False  \n",
      "32           NaN      False  \n",
      "45           NaN      False  \n",
      "46           NaN      False  \n",
      "48           NaN      False  \n",
      "82           NaN      False  \n",
      "88           NaN      False  \n",
      "103          NaN      False  \n",
      "111            X      False  \n"
     ]
    }
   ],
   "source": [
    "# Display the rows with bad addresses\n",
    "\n",
    "print(bad_addr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that four rows have The University of Texas at Austin instead of an address, four rows have addresses outside the US calling for a different format, one row is a P.O. Box, and one row starts with a decimal point. \n",
    "\n",
    "From the overall analysis of the address portion of the data set, I would say we have a healthy dataset. We do we have 10 rows that present information that isn't valid, but of those ten, four are addresses outside of the US that should be flagged in some way or formatted differently so they don't present as invalid information. We have four rows with the address listed as the university, in this case there should be constraints applied so when the user is inputting the data it has to begin with a number and be valid address and not a location name. The same restriction, address beginning with a number, could be applied to the rows containin a P.O. Box as an address and an address starting with a decimal point. \n",
    "\n",
    "Given the results we find ourselves with a dataset that has about 90% of addresses being valid, with solutions to increase the validity noted above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quality/Validity of Emails and Phone Numbers\n",
    "\n",
    "We can now continue with the next task of analyzing the health of emails and phone number in our data. \n",
    "\n",
    "For this we will analyze the `EMAIL`, `BAD_EMAIL_FLG`, `PHONE`, and `BAD_PHONE_FLG` columns in new data set `dqs_em_ph`, and begin by analyzing the number of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EMAIL              2\n",
       "BAD_EMAIL_FLG    111\n",
       "PHONE             16\n",
       "BAD_PHONE_FLG    113\n",
       "dtype: int64"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dqs_em_ph data set\n",
    "\n",
    "dqs_em_ph = dqs_data[['EMAIL', 'BAD_EMAIL_FLG', 'PHONE', 'BAD_PHONE_FLG']]\n",
    "\n",
    "# Number of missing values\n",
    "\n",
    "# Find the total number of missing values by column\n",
    "\n",
    "dqs_em_ph.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have some missing data, but as mentioned before, columns `BAD_EMAIL_FLG` and `BAD_PHONE_FLG` don't denote missing data just that the data contained in that row isn't bad. In the columns `EMAIL` and `PHONE` we do see 2 and 16 missing values respectively. \n",
    "\n",
    "Given a total of 116 observations, that leaves us with 98% valid emails and 86% valid phone numbers, with 5 emails and 3 phone number flagged as no longer working. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality/Validity by Entity Type\n",
    "\n",
    "In our dataset we have a column `ENTITY_TYPE`, detailing they type of each individual. We analyze the health of each of these entities. We will begin by splitting the dataset into its respective entity type and analyzing the health of addresses, emails, and phone numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FRIEND', 'ALUMNI', 'PARENT'], dtype=object)"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unique entries in ENTITY_TYPE\n",
    "\n",
    "dqs_data.ENTITY_TYPE.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that each individual is either a friend, alumni, or parent. Let's separate each group and analyze them for data health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creat three different data sets by ENTITY_TYPE\n",
    "\n",
    "dqs_friend = dqs_data[dqs_data['ENTITY_TYPE'] == 'FRIEND']\n",
    "\n",
    "dqs_alum = dqs_data[dqs_data['ENTITY_TYPE'] == 'ALUMNI']\n",
    "\n",
    "dqs_parent = dqs_data[dqs_data['ENTITY_TYPE'] == 'PARENT']\n",
    "\n",
    "\n",
    "# Separate our rows of interest\n",
    "\n",
    "dqs_friend = dqs_friend[['ADDRESS_LINE_1', 'ADDRESS_LINE_2', 'CITY', 'STATE', 'ZIP', 'BAD_ADDR_FLG',\n",
    "                         'EMAIL', 'BAD_EMAIL_FLG', 'PHONE', 'BAD_PHONE_FLG']]\n",
    "\n",
    "dqs_alum = dqs_alum[['ADDRESS_LINE_1', 'ADDRESS_LINE_2', 'CITY', 'STATE', 'ZIP', 'BAD_ADDR_FLG',\n",
    "                     'EMAIL', 'BAD_EMAIL_FLG', 'PHONE', 'BAD_PHONE_FLG']]\n",
    "\n",
    "dqs_parent = dqs_parent[['ADDRESS_LINE_1', 'ADDRESS_LINE_2', 'CITY', 'STATE', 'ZIP', 'BAD_ADDR_FLG',\n",
    "                         'EMAIL', 'BAD_EMAIL_FLG', 'PHONE', 'BAD_PHONE_FLG']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split the data set by `ENTITY_TYPE` we can anlyze each for data health."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Friend Entity Type\n",
    "\n",
    "We will begin by analyzing the friend entity type for data quality/health."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(63, 10)"
      ]
     },
     "execution_count": 311,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observations in FRIEND ENTITY_TYPE\n",
    "\n",
    "dqs_friend.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADDRESS_LINE_1     0\n",
       "ADDRESS_LINE_2    52\n",
       "CITY               0\n",
       "STATE              1\n",
       "ZIP                4\n",
       "BAD_ADDR_FLG      61\n",
       "EMAIL              2\n",
       "BAD_EMAIL_FLG     61\n",
       "PHONE              7\n",
       "BAD_PHONE_FLG     61\n",
       "dtype: int64"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of missing values in FRIEND ENTITY_TYPE\n",
    "\n",
    "dqs_friend.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 63 observations with no missing addresses, one missing state, four missing zips, two missing emails and seven missing phones. With flags for two bad addresses, two bad emails, and two bad phone numbers. \n",
    "\n",
    "Missing values for `STATE` and `ZIP` are part of the data we encountered before where the address was out of the country. \n",
    "\n",
    "Now let's check the validity of `ADDRESS_LINE_1` as we did before to check if addresses begin with numbers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 11)\n"
     ]
    }
   ],
   "source": [
    "# Create new column good_addr to denote addresses starting with a number\n",
    "\n",
    "dqs_friend['good_addr'] = dqs_friend['ADDRESS_LINE_1'].str[0].str.isdigit()\n",
    "\n",
    "\n",
    "# Isolate rows which address doesn't start with a number\n",
    "\n",
    "bad_addr_fr = dqs_friend[dqs_friend['good_addr'] == False]\n",
    "\n",
    "# Print number of rows have bad address in dqs_friend\n",
    "\n",
    "print(bad_addr_fr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that in the friend entity type, we have 9 out of the 10 rows from our total data set with invalid addresses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alum Entity Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37, 10)"
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observations in ALUMNI ENTITY_TYPE\n",
    "\n",
    "dqs_alum.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADDRESS_LINE_1     0\n",
       "ADDRESS_LINE_2    25\n",
       "CITY               0\n",
       "STATE              0\n",
       "ZIP                0\n",
       "BAD_ADDR_FLG      34\n",
       "EMAIL              0\n",
       "BAD_EMAIL_FLG     35\n",
       "PHONE              1\n",
       "BAD_PHONE_FLG     36\n",
       "dtype: int64"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of missing values in ALUMNI ENTITY_TYPE\n",
    "\n",
    "dqs_alum.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 37 observations with only one missing phone number, and flags for three bad addresses, two bad emails, and one bad phone number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 11)\n"
     ]
    }
   ],
   "source": [
    "# Create new column good_addr to denote addresses starting with a number\n",
    "\n",
    "dqs_alum['good_addr'] = dqs_alum['ADDRESS_LINE_1'].str[0].str.isdigit()\n",
    "\n",
    "\n",
    "# Isolate rows which address doesn't start with a number\n",
    "\n",
    "bad_addr_al = dqs_alum[dqs_alum['good_addr'] == False]\n",
    "\n",
    "# Print number of rows have bad address in dqs_friend\n",
    "\n",
    "print(bad_addr_al.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the alumni entity type we find one row with an address not beginning with a number, a much more healhty data set when compared to the friend entity type."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parent Entity Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 10)"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observations in PARENT ENTITY_TYPE\n",
    "\n",
    "dqs_parent.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADDRESS_LINE_1     0\n",
       "ADDRESS_LINE_2    12\n",
       "CITY               0\n",
       "STATE              0\n",
       "ZIP                0\n",
       "BAD_ADDR_FLG      15\n",
       "EMAIL              0\n",
       "BAD_EMAIL_FLG     15\n",
       "PHONE              8\n",
       "BAD_PHONE_FLG     16\n",
       "dtype: int64"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of missing values in ALUMNI ENTITY_TYPE\n",
    "\n",
    "dqs_parent.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a total of 16 observations with eight missing phone numbers, one bad address flag, and one bad email flag. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, 11)\n"
     ]
    }
   ],
   "source": [
    "# Create new column good_addr to denote addresses starting with a number\n",
    "\n",
    "dqs_parent['good_addr'] = dqs_parent['ADDRESS_LINE_1'].str[0].str.isdigit()\n",
    "\n",
    "\n",
    "# Isolate rows which address doesn't start with a number\n",
    "\n",
    "bad_addr_pr = dqs_parent[dqs_parent['good_addr'] == False]\n",
    "\n",
    "# Print number of rows have bad address in dqs_friend\n",
    "\n",
    "print(bad_addr_pr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the parent entity type we find zero invalid addresses. Making it the healthiest when compared to the friend and alumni entity types. With the friend entity type being the least healthy. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Anomalies\n",
    "\n",
    "After analysis we concluded that just around 90% of our data is valid/healthy data, and the parent entity type is the group with the healthiest and the friend entity type with least healthy data. \n",
    "\n",
    "One more thing I would like to note that affects data quality, is the data types found within our data. Our data is made up of letters, words, and numbers. An important aspect of quality data is having it labeled as the correct type to make analyzing the data easier. \n",
    "\n",
    "Letters and words are known as string data, and numbers can be either integers (whole numbers) like age, or floats for numbers with decimal places. At the begining of our analysis, during the data exploration we saw that a lot of columns are read in as objects instead of strings or numbers. This can cause some operations to not function properly. Moreover columns like addresses and phone numbers should be labeled as strings given that mathematical operations with these columns are not possible. Setting restrictions as to the data type of each column when building out the database, will allow for an easier analys of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
